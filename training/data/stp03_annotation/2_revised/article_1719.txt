Tudo sobre Twitter

Alguns indiv√≠duos desacreditam dos avisos de especialistas a respeito dos perigos √©ticos da intelig√™ncia artificial - por√©m, agora temos uma prova bem cr√≠tica de como um algoritmo de aprendizado de m√°quina pode ser maligno mesmo sem ter sido programado propositalmente para isso.

O Twitter, neste momento, est√° quebrando a cabe√ßa para entender o porqu√™ de seu sistema de recorte autom√°tico de fotografias estar demonstrando tend√™ncias racistas.

O algoritmo em quest√£o foi projetado para otimizar a visualiza√ß√£o de imagens no feed da rede social.

Quando voc√™ publica uma foto grande demais e/ou fora das propor√ß√µes aceitas pela plataforma, ela automaticamente escolhe uma por√ß√£o do retrato para exibir, priorizando rostos humanos que porventura sejam identificados - √© necess√°rio clicar na imagem para visualiz√°-la por completo.

At√© a√≠, tudo bem.

O problema √© que alguns internautas perceberam que esse sistema prioriza, na maioria das vezes, rostos de pessoas brancas em vez de indiv√≠duos negros.

A pol√™mica come√ßou quando o programador Tony Arcieri fez uma s√©rie de testes em seu perfil, compartilhando montagens que juntavam os rostos do senador Mitch McConnel e do ex-presidente Barack Obama.

O sistema sempre priorizava o rosto de Mitch.

Trying a horrible experiment... Which will the Twitter algorithm pick: Mitch McConnell or Barack Obama? pic.twitter.com/bR1GRyCkia - Tony 'Abolish (Pol)ICE' Arcieri ü¶Ä (@bascule) September 19, 2020 Graham Christensen, integrante do time de seguran√ßa e infraestrutura do NixOS, realizou outro teste com imagens stock (tiradas de bibliotecas) para garantir que o problema n√£o estaria ligado a outros fatores externos, como uma eventual maior popularidade do senador em tempos de discuss√£o pol√≠tica pr√©-elei√ß√µes presidenciais dos EUA.

Infelizmente, a tentativa foi em v√£o, pois o c√≥digo novamente priorizou o modelo branco em todos os cen√°rios.

I wonder how it is that you've said this without testing it? pic.twitter.com/rro1vn8Mh8 - Graham Christensen (@grhmc) September 19, 2020 Acredite ou n√£o, mas alguns internautas chegaram ao ponto de testar o algoritmo com personagens de desenhos animados (Lenny e Carl, da franquia

Os Simpsons) e at√© mesmo com cachorros.

Voc√™ j√° imagina o resultado: Lenny ficou em destaque, tal como o c√£o de pelagem clara versus aquele de pelagem escura.

I wonder if Twitter does this to fictional characters too.

Lenny

Carl pic.twitter.com/fmJMWkkYEf - Jordan Simonovski (@_jsimonovski)

September 20, 2020

Double checking just in case it depends on the order of the pictures pic.twitter.com/1oHJE0bZHk - Ant üå∏ (@acheepcheep)

September 20, 2020

Como resolver? Naturalmente, essa situa√ß√£o n√£o est√° nada legal para o Twitter.

Alguns executivos da companhia come√ßaram a fazer seus pr√≥prios testes n√£o-cient√≠ficos e n√£o-oficiais para tentar entender o porqu√™ desse comportamento.

Dantley Davis, chefe do setor de design da rede social, por exemplo, conseguiu reverter um dos testes dos internautas ao esconder as m√£os do modelo negro.e vesti-lo com o mesmo terno do modelo branco.

Here's another example of what I've experimented with.

It's not a scientific test as it's an isolated example, but it points to some variables that we need to look into.

Both men now have the same suits and I covered

their hands.

We're still investigating the NN.

pic.twitter.com/06BhFgDkyA - Dantley üî•‚úäüèæüíô (@dantley) September 20, 2020 Liz Kelley, da equipe de comunica√ß√µes da empresa, afirmou que a companhia 'n√£o encontrou evid√™ncias de prefer√™ncias raciais ou de g√™nero em seus testes', mas concordou que seriam necess√°rias an√°lises mais aprofundadas 'Esta √© uma quest√£o muito importante. Para evitar isso, fizemos uma an√°lise em nosso modelo quando o lan√ßamos, mas ele precisa de melhorias cont√≠nuas. Adoro este teste p√∫blico, aberto e rigoroso - e estou ansioso por aprender com ele', finaliza Parag Agrawal, gerente de tecnologia do Twitter.

Posicionamento oficial do Twitter

A assessoria de imprensa do Twitter no Brasil entrou em contato com o Canaltech e incluiu o seguinte posicionamento oficial, publicado tamb√©m no perfil nacional da companhia no microblog: Fizemos uma s√©rie de testes antes de lan√ßar o modelo e n√£o encontramos evid√™ncias de preconceito racial ou de g√™nero.

Est√° claro que temos mais an√°lises a fazer.

Continuaremos compartilhando nossos aprendizados e medidas, abriremos o c√≥digo para que outros possam revis√°-lo.

Fonte: The Verge


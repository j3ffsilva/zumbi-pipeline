Reconhecimento facial que prejudica rostos negros reflete racismo de quem programa sites e plataformas, explica Carla Vieira, do perifaCode

O perifaCode √© uma comunidade de programa√ß√£o para pessoas de regi√£o ou origem perif√©rica ‚Äì da favela pro mundo |

Foto: Divulga√ß√£o

Se voc√™ tem redes sociais talvez j√° deva ter percebido que rostos de pessoas brancas s√£o mais frequentes nas plataformas como Twitter e Instagram.

No Twitter, por exemplo, postando uma imagem com fotos de uma pessoa branca e de uma pessoa negra, independentemente da posi√ß√£o, √© o rosto da pessoa branca que ganhar√° destaque.

Para pesquisadoras e especialistas essa situa√ß√£o tem nome: racismo algor√≠tmico.

A engenheira de software Carla Vieira, bacharel em Sistemas de Informa√ß√£o e mestranda em Intelig√™ncia Artificial pela USP (Universidade de S√£o Paulo), que tamb√©m √© co-organizadora da perifaCode, comunidade que busca levar a tecnologia para dentro das periferias, √© um dos nomes que pesquisam o tema no Brasil.

Ela conta que o assunto causa confus√£o, dentro e fora do mundo das tecnologias, mas, que cada vez mais, pesquisas sobre o assunto s√£o feitas. 'Para as pessoas especialistas na √°rea, a confus√£o est√° na alega√ß√£o de que o algoritmo n√£o √© racista, porque ele √© uma matem√°tica, √© baseado em estat√≠stica e fundamentado em matem√°tica e n√£o tem como ser racista, quem s√£o racistas s√£o as pessoas que programam os algoritmos', explica.

Veja: Cuidar da sa√∫de mental de negros e LGBTs exige combater racismo e LGBTfobia

Em sua pesquisa, Carla debate exatamente isso: 'A gente usa esse termo justamente por que o racismo √© uma estrutura social, de um sistema que oprime pessoas negras. O racismo est√° na pessoa que programa e o algoritmo tem sido uma ferramenta do racismo. √â como o racismo consegue se perpetuar atrav√©s da tecnologia como ferramenta'. O racismo algor√≠tmico est√° longe de existir apenas nas postagens das redes sociais.

O reconhecimento facial, explica Carla, chama mais aten√ß√£o porque √© mais impactante 'O algoritmo do Google Fotos categorizar uma pessoa negra como um gorila ou como aconteceu no Twitter da ferramenta de recorte ter esse vi√©s. √â mais impactante, mas n√£o √© s√≥ isso'. Vamos fazer uma thread com 20 artigos sobre racismo algor√≠tmico, vis√£o computacional e reconhecimento facial? Chega a√≠ -> pic.twitter.com/rZkbhl51Xs - Tarcizio Silva (@tarciziosilva) September 20, 2020

Tamb√©m √© essa forma de racismo que faz com que produtores de conte√∫do negros tenham menos alcance do que pessoas brancas nas plataformas digitais, al√©m de definir quem deve ser punido nesses espa√ßos 'Algumas plataformas usam algoritmos para gera√ß√£o de conte√∫do, como o Facebook e o Instagram. Eles t√™m pessoas reais que ficam ali olhando conte√∫dos diariamente para retirar da plataforma, como pornografia, e tamb√©m tem algoritmos que est√£o treinados para fazer isso de forma mais automatizada, que √© muito mais barato', explica Carla.

Veja: Instagram censura artistas com conte√∫dos antirracistas 'Mas essa modera√ß√£o de conte√∫do prejudica muito pessoas negras. No Instagram j√° aconteceu com v√°rias artistas que falam sobre favela e sobre suas viv√™ncias ter foto ou ilustra√ß√£o banida por dizer que falava sobre viol√™ncia. Foto de uma pessoa negra, de √≥culos, na favela √© considerada uma foto que viola os termos', aponta.

Para al√©m das buscas de imagens no Google e nas redes sociais, o racismo algor√≠tmico tamb√©m define quem deve ter prioridade na fila de um hospital 'Nos EUA, por exemplo, algoritmos fazem an√°lises para priorizar pessoas nos hospitais. Eles usavam hist√≥ricos m√©dicos para analisar se a pessoa tem prioridade ou n√£o na fila dos hospitais', exemplifica a engenheira digital 'Pessoas negras n√£o passam nos hospitais nos EUA por motivos sociais e o algoritmo entendia que n√£o deveria priorizar. Quem programou n√£o pensou que havia uma quest√£o de ra√ßa para analisar. √â algo que poderia ter sido facilmente evitado e n√£o foi porque a maioria das pessoas que programam essas tecnologias s√£o homens brancos, que n√£o pensam nesse tipo de situa√ß√£o', completa.

Outras duas situa√ß√µes refor√ßam o argumento de Carla.

A primeira, apontada por Tarc√≠zio Silva, mestre em Comunica√ß√£o e Cultura Contempor√¢neas pela UFB, em reportagem do portal Geled√©s, aconteceu em Londres onde 'mais de 80% das abordagens incentivadas por reconhecimento facial foram erradas'. J√° segundo reportagem do El Pa√≠s, na Nova Zel√¢ndia, especialistas desenvolveram um sistema que lista uma s√©rie de variantes (idade dos pais, sa√∫de mental, antecedentes criminais) para determinar quais rec√©m-nascidos tinham maior risco de serem maltratados at√© os 5 anos de idade.

Lan√ßado em 2014, o programa foi encerrado no ano seguinte, gra√ßas a uma investiga√ß√£o que demonstrou que o sistema errava em 70% dos casos.

Veja: Com Racismo

N√£o H√° Democracia: movimento busca visibilidade para quest√£o racial

Uma forma de resolver esse problema, defende Carla, √© inserindo pessoas negras, perif√©ricas, mulheres e LGBTs na programa√ß√£o desses sistemas e nesses espa√ßos que s√£o predominantemente compostos por homens cisg√™neros, h√©teros e brancos 'Precisamos incluir pessoas que s√£o historicamente impactadas por esses algoritmos, pensar em como desenvolver tecnologia e ferramenta que resolva os problemas das pessoas e n√£o crie mais problemas' 'A gente s√≥ vai resolver o problema quando pararmos de pensar em vieses inconscientes e incluirmos essas pessoas na constru√ß√£o desses algoritmos, tanto popula√ß√£o e usu√°rios quanto pesquisadores, com mais pessoas negras pesquisando algoritmos, mais pessoas negras desenvolvendo tecnologias e produtos', argumenta.

Apoie a Ponte! Foi com essa ideia que nasceu o perifaCode.

Feito para unir as pessoas perif√©ricas e negras que j√° trabalham na √°rea da tecnologia, o projeto surgiu com a necessidade de dialogar o dia a dia desses profissionais. 'A gente tem carrega outros pesos, al√©m da profiss√£o, por sermos de uma minoria de tamb√©m cuida da fam√≠lia e resolve outros problemas'. Outro lado

A reportagem procurou o Instagram.

Em texto publicado no blog da empresa, o diretor da rede social, Adam Mosseri, afirma que, entre outras medidas, foi criada uma Equipe de Equidade do Instagram 'que se concentrar√° em entender as vieses no desenvolvimento de nossos produtos e nas experi√™ncias das pessoas no Instagram, assim como lidar com elas'. Procurado sobre o tema, o Twitter enviou a seguinte resposta: Fizemos uma s√©rie de testes antes de lan√ßar o modelo e n√£o encontramos evid√™ncias de preconceito racial ou de g√™nero.

Est√° claro que temos mais an√°lises a fazer.

Continuaremos compartilhando nossos aprendizados e medidas, e abriremos o c√≥digo para que outros possam revis√°-lo.

https://t.co/yZphB9pK5p - Twitter Brasil em üè† (@TwitterBrasil)

September 21, 2020

ATUALIZA√á√ÉO: a reportagem foi modificada no dia 25/09/2020 √†s 19h10 para contemplar o posicionamento das empresas citadas na mat√©ria


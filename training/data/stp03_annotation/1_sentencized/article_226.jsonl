{"text":"NEW YORK."}
{"text":"Os países devem assegurar que o uso de inteligência artificial por seus serviços de segurança, especialmente os algoritmos utilizados para reconhecimento facial ou controle policial de fronteiras, não reforcem preconceitos estruturais."}
{"text":"A afirmação foi feita pela Organização das Nações Unidas (ONU), após o lançamento de um relatório de seu Comitê para Eliminação da Discriminação Racial."}
{"text":"Entrevista:’Algoritmos têm responsabilidade pela violência contra mulheres e pessoas negras"}
{"text":"’, diz pesquisadora da Universidade da Califórnia —"}
{"text":"Existe um grande risco de que (a inteligência artificial) reforce o preconceito e, portanto, agrave ou possibilite práticas discriminatórias — alertou Verene Shepherd, especialista em direitos humanos, em entrevista à agência de notícias AFP."}
{"text":"Jamaicana, ela é historiadora e liderou a equipe de 18 analistas independentes que redigiu o documento em que fazem recomendações aos 182 países que ratificaram im tratado internacional proibindo a discriminação racial."}
{"text":"O relatório descreve a preocupação da ONU com a perpetuação do racismo estrutural pelos algoritmos, além de propor maneiras de lidar com o problema."}
{"text":"Segundo o documento, sistemas de vigilância que usam inteligência artificial e supostamente apoiam a prevenção de crimes foram implementados pela primeira vez nos Estados Unidos em meados da década de 2000, mas reforçam preconceitos contra comunidades específicas, como mulheres, negros, hispânicos e muçulmanos."}
{"text":"Além disso, o comitê afirma que os algoritmos são responsáveis por um 'bombardeio de publicidade preconceituosa' que os usuários de ferramentas de busca ou de redes sociais recebem."}
{"text":"—"}
{"text":"Os dados históricos sobre prisões em um bairro determinado (que alimentam a inteligência artificial) podem refletir muito bem as práticas policiais preconceituosas e, consequentemente, reproduzi-las."}
{"text":"Esses dados aumentam o risco de um excesso de presença policial que poderia levar a realizar mais prisões e, desse modo, criar um ciclo vicioso."}
{"text":"Dados incorretos provocam maus resultados — alerta Shepherd."}
{"text":"Entrevista:‘Algoritmos são racistas’, diz fundadora do PretaLab"}
{"text":"Em seu relatório, o comitê demonstra preocupação pelo uso generalizado do reconhecimento facial."}
{"text":"Estudos já demonstraram que esses dados têm dificuldades para reconhecer rostos de pele escura ou de mulheres."}
{"text":"—"}
{"text":"Nós ouvimos sobre empresas que usam esses algoritmo para discriminar com base na cor da pele."}
{"text":"Ativistas reclamam sobre o uso cada vez mais difundido de inteligência artifical, reconhecimento facial e outras tecnologias."}
{"text":"É muito usado nos EUA, onde já ouvimos reclamações das comunidades negras, e também na União Europeia."}
{"text":"Na América Latina, cidadão negros e povos indígenas dizem o mesmo — afirmou Shepherd à agência de notícias Reuters, citando Brasil e Colômbia como dois países que discriminam seus cidadãos usando esse tipo de tecnologia."}
{"text":"— Esses são lugares os casos prevalecem mais, ouvimos muitos casos."}
{"text":"Apps menstruais:por que as mulheres devem estar atentas?"}
{"text":"O comitê pede aos países que regulamentem as empresas que trabalham com esse tipo de tecnologia para garantir que a legislação internacional sobre direitos humanos seja respeitada."}
{"text":"Entre as recomendações está a necessidade de transparência na concepção e na aplicação desses direitos."}
{"text":"O comitê também reforça que o racismo não foi iniciado por essas tecnologias e que movimentos como o Black Lives Matter e campanhas em defesa de grupos vulneráveis reforçam a necessidade dessas recomendações."}

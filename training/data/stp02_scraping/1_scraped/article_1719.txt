Tudo sobre Twitter

Alguns indivÃ­duos desacreditam dos avisos de especialistas a respeito dos perigos Ã©ticos da inteligÃªncia artificial â€” porÃ©m, agora temos uma prova bem crÃ­tica de como um algoritmo de aprendizado de mÃ¡quina pode ser maligno mesmo sem ter sido programado propositalmente para isso. O Twitter, neste momento, estÃ¡ quebrando a cabeÃ§a para entender o porquÃª de seu sistema de recorte automÃ¡tico de fotografias estar demonstrando tendÃªncias racistas.

O algoritmo em questÃ£o foi projetado para otimizar a visualizaÃ§Ã£o de imagens no feed da rede social. Quando vocÃª publica uma foto grande demais e/ou fora das proporÃ§Ãµes aceitas pela plataforma, ela automaticamente escolhe uma porÃ§Ã£o do retrato para exibir, priorizando rostos humanos que porventura sejam identificados â€” Ã© necessÃ¡rio clicar na imagem para visualizÃ¡-la por completo. AtÃ© aÃ­, tudo bem.

O problema Ã© que alguns internautas perceberam que esse sistema prioriza, na maioria das vezes, rostos de pessoas brancas em vez de indivÃ­duos negros. A polÃªmica comeÃ§ou quando o programador Tony Arcieri fez uma sÃ©rie de testes em seu perfil, compartilhando montagens que juntavam os rostos do senador Mitch McConnel e do ex-presidente Barack Obama. O sistema sempre priorizava o rosto de Mitch.

Trying a horrible experiment...



Which will the Twitter algorithm pick: Mitch McConnell or Barack Obama? pic.twitter.com/bR1GRyCkia â€” Tony â€œAbolish (Pol)ICEâ€ Arcieri ğŸ¦€ (@bascule) September 19, 2020

Graham Christensen, integrante do time de seguranÃ§a e infraestrutura do NixOS, realizou outro teste com imagens stock (tiradas de bibliotecas) para garantir que o problema nÃ£o estaria ligado a outros fatores externos, como uma eventual maior popularidade do senador em tempos de discussÃ£o polÃ­tica prÃ©-eleiÃ§Ãµes presidenciais dos EUA. Infelizmente, a tentativa foi em vÃ£o, pois o cÃ³digo novamente priorizou o modelo branco em todos os cenÃ¡rios.

I wonder how it is that you've said this without testing it? pic.twitter.com/rro1vn8Mh8 â€” Graham Christensen (@grhmc) September 19, 2020

Acredite ou nÃ£o, mas alguns internautas chegaram ao ponto de testar o algoritmo com personagens de desenhos animados (Lenny e Carl, da franquia Os Simpsons) e atÃ© mesmo com cachorros. VocÃª jÃ¡ imagina o resultado: Lenny ficou em destaque, tal como o cÃ£o de pelagem clara versus aquele de pelagem escura.

I wonder if Twitter does this to fictional characters too.



Lenny Carl pic.twitter.com/fmJMWkkYEf â€” Jordan Simonovski (@_jsimonovski) September 20, 2020

Double checking just in case it depends on the order of the pictures pic.twitter.com/1oHJE0bZHk â€” Ant ğŸŒ¸ (@acheepcheep) September 20, 2020

Como resolver?

Naturalmente, essa situaÃ§Ã£o nÃ£o estÃ¡ nada legal para o Twitter. Alguns executivos da companhia comeÃ§aram a fazer seus prÃ³prios testes nÃ£o-cientÃ­ficos e nÃ£o-oficiais para tentar entender o porquÃª desse comportamento. Dantley Davis, chefe do setor de design da rede social, por exemplo, conseguiu reverter um dos testes dos internautas ao esconder as mÃ£os do modelo negro.e vesti-lo com o mesmo terno do modelo branco.

Here's another example of what I've experimented with. It's not a scientific test as it's an isolated example, but it points to some variables that we need to look into. Both men now have the same suits and I covered their hands. We're still investigating the NN. pic.twitter.com/06BhFgDkyA â€” Dantley ğŸ”¥âœŠğŸ¾ğŸ’™ (@dantley) September 20, 2020

Liz Kelley, da equipe de comunicaÃ§Ãµes da empresa, afirmou que a companhia â€œnÃ£o encontrou evidÃªncias de preferÃªncias raciais ou de gÃªnero em seus testesâ€, mas concordou que seriam necessÃ¡rias anÃ¡lises mais aprofundadas.

â€œEsta Ã© uma questÃ£o muito importante. Para evitar isso, fizemos uma anÃ¡lise em nosso modelo quando o lanÃ§amos, mas ele precisa de melhorias contÃ­nuas. Adoro este teste pÃºblico, aberto e rigoroso â€” e estou ansioso por aprender com eleâ€, finaliza Parag Agrawal, gerente de tecnologia do Twitter.

Posicionamento oficial do Twitter

A assessoria de imprensa do Twitter no Brasil entrou em contato com o Canaltech e incluiu o seguinte posicionamento oficial, publicado tambÃ©m no perfil nacional da companhia no microblog:

Fizemos uma sÃ©rie de testes antes de lanÃ§ar o modelo e nÃ£o encontramos evidÃªncias de preconceito racial ou de gÃªnero. EstÃ¡ claro que temos mais anÃ¡lises a fazer. Continuaremos compartilhando nossos aprendizados e medidas, abriremos o cÃ³digo para que outros possam revisÃ¡-lo.

Fonte: The Verge